{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate data: 400\n",
      "validate labels:400\n",
      "train data:1600\n",
      "train labels:1600\n",
      "{'train_labels': matrix([[ 1.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1., ...,  0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1., ...,  0.,  0.,  0.],\n",
      "        ..., \n",
      "        [ 1.,  1.,  1., ...,  0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1., ...,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float16), 'train_data': matrix([[  6.2109375 ,   1.68554688,   2.2421875 , ...,   5.984375  ,\n",
      "           2.64453125,   3.72070312],\n",
      "        [  5.25390625,   5.046875  ,   2.48828125, ...,   6.39453125,\n",
      "           8.03125   ,   5.640625  ],\n",
      "        [  6.1484375 ,   9.0234375 ,   3.37890625, ...,   9.1875    ,\n",
      "          11.6953125 ,   5.5234375 ],\n",
      "        ..., \n",
      "        [  2.94921875,   4.578125  ,   3.703125  , ...,   2.609375  ,\n",
      "           2.49023438,   3.62304688],\n",
      "        [  4.16015625,   7.5       ,   2.8203125 , ...,   4.97265625,\n",
      "           7.36328125,   5.703125  ],\n",
      "        [  3.85351562,   2.8046875 ,   2.20703125, ...,   4.74609375,\n",
      "           2.10742188,   5.05078125]], dtype=float16), 'validate_data': matrix([[ 3.38476562,  5.515625  ,  1.61035156, ...,  6.04296875,\n",
      "          4.328125  ,  3.33984375],\n",
      "        [ 3.02929688,  1.91699219,  0.13842773, ...,  1.87890625,\n",
      "          0.41894531,  3.9453125 ],\n",
      "        [ 3.83789062,  6.30078125,  2.68359375, ...,  1.96777344,\n",
      "          3.40820312,  2.09960938],\n",
      "        ..., \n",
      "        [ 3.75      ,  3.609375  ,  0.47729492, ...,  8.6953125 ,\n",
      "          2.00976562,  4.046875  ],\n",
      "        [ 5.3828125 ,  7.34375   ,  1.04785156, ...,  2.34179688,\n",
      "          3.6953125 ,  2.08789062],\n",
      "        [ 4.60546875,  0.82958984,  0.72460938, ...,  5.81640625,\n",
      "          0.04830933,  1.16699219]], dtype=float16), 'validate_labels': matrix([[ 1.,  1.,  1., ...,  0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1., ...,  1.,  0.,  0.],\n",
      "        [ 1.,  1.,  1., ...,  0.,  0.,  0.],\n",
      "        ..., \n",
      "        [ 1.,  1.,  1., ...,  0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1., ...,  0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1., ...,  0.,  0.,  0.]], dtype=float16)}\n"
     ]
    }
   ],
   "source": [
    "# Goal - convert features and labels from dataframes to np arrays ready for use in tensorflow.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "tf_output = {}\n",
    "\n",
    "# Load the cached pandas pickle file.\n",
    "base_dir = '../data/'\n",
    "\n",
    "# Load validation set dataframe from file\n",
    "feat_validate_df = pd.read_pickle(base_dir + 'feat_validate.pickle')\n",
    "# Group the data by taking the max of all images per business\n",
    "biz_feat_validate_df= feat_validate_df.groupby(['business_id']).agg(['max'])\n",
    "# Drop photo_id, since all ids were averaged (And we don't care about it anyways)\n",
    "biz_feat_validate_df.drop('photo_id', axis=1, inplace=True)\n",
    "# Grab the (already sorted) business_ids to be used to split the labels up.\n",
    "biz_ids_validate = biz_feat_validate_df.index\n",
    "\n",
    "# See above for details.\n",
    "feat_train_df = pd.read_pickle(base_dir + 'feat_train.pickle')\n",
    "biz_feat_train_df= feat_train_df.groupby(['business_id']).agg(['max'])\n",
    "biz_feat_train_df.drop('photo_id', axis=1, inplace=True)\n",
    "biz_ids_train = biz_feat_train_df.index\n",
    "\n",
    "\n",
    "#Clean up labels\n",
    "labels_df = pd.read_pickle(base_dir + 'labels.pickle')\n",
    "# Make the index int64 instead of string so sorting and matching works correctly.\n",
    "labels_df.index = pd.to_numeric(labels_df.index)\n",
    "# Sort all the labels based on business_id\n",
    "labels_df = labels_df.sort_index()\n",
    "\n",
    "tf_output['validate_data'] = np.matrix(biz_feat_validate_df)\n",
    "tf_output['validate_labels'] = np.matrix(labels_df[labels_df.index.isin(biz_ids_validate)], dtype='float16')\n",
    "tf_output['train_data'] = np.matrix(biz_feat_train_df)\n",
    "tf_output['train_labels'] = np.matrix(labels_df[labels_df.index.isin(biz_ids_train)], dtype='float16')\n",
    "\n",
    "print \"validate data: \" + str(len(tf_output['validate_data']))\n",
    "print \"validate labels:\" + str(len(tf_output['validate_labels']))\n",
    "print \"train data:\" + str(len(tf_output['train_data']))\n",
    "print \"train labels:\" + str(len(tf_output['train_labels']))\n",
    "print tf_output\n",
    "\n",
    "output_file = open(base_dir + \"tf_data.pickle\", 'wb')\n",
    "pickle.dump(tf_output, output_file)\n",
    "output_file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
