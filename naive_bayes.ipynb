{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import multiclass as mc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, scale\n",
    "from lib import accuracy\n",
    "from lib import labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (1600, 1024) (1600, 9)\n",
      "Validation set (400, 1024) (400, 9)\n",
      "Test set (10000, 1024) 10000\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/\"\n",
    "pickle_file = 'tf_data_mean.pickle'\n",
    "\n",
    "with open(data_dir + pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = np.matrix(save['train_data'], dtype='float32')\n",
    "    train_labels = np.matrix(save['train_labels'], dtype='float32')\n",
    "    valid_dataset = np.matrix(save['validate_data'], dtype='float32')\n",
    "    valid_labels = np.matrix(save['validate_labels'], dtype='float32')\n",
    "    test_dataset = np.matrix(save['test_data'], dtype='float32')\n",
    "    test_bids = list(save['test_business_ids'])\n",
    "    #test_bids = np.ravel(test_bids)\n",
    "    del save  # hint to help gc free up memory\n",
    "    print 'Training set', train_dataset.shape, train_labels.shape\n",
    "    print 'Validation set', valid_dataset.shape, valid_labels.shape\n",
    "    print 'Test set', test_dataset.shape, len(test_bids)\n",
    "\n",
    "# Convert labels to a dict of binarized labels [1. if true, 1. if false]\n",
    "# So can be used for softmax per label.\n",
    "#train_labels = labels.binarize_softmax_labels(train_labels)\n",
    "#valid_labels = labels.binarize_softmax_labels(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the percentage of labels?\n",
      "#0: 33.6875\n",
      "##: 33.0\n",
      "\n",
      "#1: 49.25\n",
      "##: 51.25\n",
      "\n",
      "#2: 50.9375\n",
      "##: 52.75\n",
      "\n",
      "#3: 49.875\n",
      "##: 51.25\n",
      "\n",
      "#4: 27.625\n",
      "##: 26.25\n",
      "\n",
      "#5: 62.125\n",
      "##: 63.75\n",
      "\n",
      "#6: 67.875\n",
      "##: 68.5\n",
      "\n",
      "#7: 28.6875\n",
      "##: 28.25\n",
      "\n",
      "#8: 62.1875\n",
      "##: 60.75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"What's the percentage of labels?\"\n",
    "for i in range(9):\n",
    "    print \"#\"+str(i)+\":\", np.sum(train_labels[:,i]) / 16\n",
    "    print \"##:\", np.sum(valid_labels[:,i]) / 4\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0 : 0.6775\n",
      "# 1 : 0.83\n",
      "# 2 : 0.8625\n",
      "# 3 : 0.545\n",
      "# 4 : 0.815\n",
      "# 5 : 0.81\n",
      "# 6 : 0.85\n",
      "# 7 : 0.8075\n",
      "# 8 : 0.8\n",
      "\n",
      "# 0 : 0.695\n",
      "# 1 : 0.8\n",
      "# 2 : 0.825\n",
      "# 3 : 0.5275\n",
      "# 4 : 0.785\n",
      "# 5 : 0.79\n",
      "# 6 : 0.82\n",
      "# 7 : 0.7825\n",
      "# 8 : 0.8025\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "#LABEL TO PREDICT:\n",
    "\n",
    "for i in range(9):\n",
    "    label_to_predict = i\n",
    "    # predicting 5 based on the other items.\n",
    "    X = np.delete(train_labels, label_to_predict, 1)\n",
    "    Y = np.ravel(train_labels[:,label_to_predict])\n",
    "    valid_X = np.delete(valid_labels, label_to_predict, 1)\n",
    "    clf = BernoulliNB()\n",
    "    clf.fit(X, Y)\n",
    "    predictions = clf.predict(valid_X)\n",
    "    \n",
    "    correct_pct = np.sum(np.equal(np.ravel(predictions), np.ravel(valid_labels[:,label_to_predict]))) * 1.0 / len(predictions)\n",
    "    print \"#\", label_to_predict, \":\",np.sum(np.equal(\n",
    "        np.ravel(predictions), np.ravel(valid_labels[:,label_to_predict])\n",
    "    )) * 1.0 / len(predictions)\n",
    "\n",
    "nb_preds = np.zeros((400, 9))\n",
    "print \"\"\n",
    "for i in range(9):\n",
    "    label_to_predict = i\n",
    "    # predicting 5 based on the other items.\n",
    "    X = np.delete(train_labels, [0, 1, 2, 3, label_to_predict], 1)\n",
    "    Y = np.ravel(train_labels[:,label_to_predict])\n",
    "    valid_X = np.delete(valid_labels, [0, 1, 2, 3, label_to_predict], 1)\n",
    "    clf = BernoulliNB()\n",
    "    clf.fit(X, Y)\n",
    "    predictions = clf.predict(valid_X)\n",
    "    nb_preds[:,i] = predictions\n",
    "    \n",
    "    correct_pct = np.sum(np.equal(np.ravel(predictions), np.ravel(valid_labels[:,label_to_predict]))) * 1.0 / len(predictions)\n",
    "    print \"#\", label_to_predict, \":\",np.sum(np.equal(\n",
    "        np.ravel(predictions), np.ravel(valid_labels[:,label_to_predict])\n",
    "    )) * 1.0 / len(predictions)\n",
    "    \n",
    "with open(data_dir+\"nb_preds.pickle\", 'wb') as f:\n",
    "    # Pickle dictionary using protocol 0.\n",
    "    pickle.dump(nb_preds, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.75"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sum() / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84499999999999997"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.equal(np.ravel(predictions), np.ravel(valid_labels[:,5]))) * 1.0 / len(predictions)\n",
    "# 84% right (compared to a dismal 56%), but that assumes the inputs are 100% correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainL = labels.matrix_2_labels(train_labels)\n",
    "\n",
    "trainL = MultiLabelBinarizer().fit_transform(trainL)\n",
    "#train_labels = matrix_2_labels(train_labels)\n",
    "trainL = np.asarray(trainL)\n",
    "\n",
    "#clf = svm.SVC(decision_function_shape='ovr', kernel='linear')\n",
    "\n",
    "\n",
    "clf = OneVsRestClassifier(svm.LinearSVC(random_state=42)).fit(train_dataset, trainL)\n",
    "svm_predictions = clf.predict(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 8)\n",
      "(1600,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting 5 based on the other items.\n",
    "X = np.delete(train_labels, label_to_predict, 1)\n",
    "Y = np.ravel(train_labels[:,label_to_predict])\n",
    "valid_X = np.delete(svm_predictions, label_to_predict, 1)\n",
    "\n",
    "print X.shape\n",
    "print Y.shape\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old occupancy: 58\n",
      "old accuracy: 0.6275\n",
      "new occupancy: 52.25\n",
      "new accuracy: 0.545\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(valid_X)\n",
    "print \"old occupancy:\", svm_predictions[:,label_to_predict].sum() / 4\n",
    "print \"old accuracy:\",np.sum(np.equal(\n",
    "        np.ravel(svm_predictions[:,label_to_predict]), np.ravel(valid_labels[:,label_to_predict])\n",
    "    )) * 1.0 / len(svm_predictions[:,label_to_predict])\n",
    "\n",
    "print \"new occupancy:\", predictions.sum() / 4\n",
    "print \"new accuracy:\",np.sum(np.equal(\n",
    "        np.ravel(predictions), np.ravel(valid_labels[:,label_to_predict])\n",
    "    )) * 1.0 / len(predictions)\n",
    "# Damn, drops down to 54.5% from 62.7%, thus it's worse\n",
    "# Still,if we can increase the other scores using the neural net, we might be able to have\n",
    "# enough correct priors to make a better prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old mean_f1_score: 0.768548180986\n",
      "new mean_f1_score: 0.752406496281\n"
     ]
    }
   ],
   "source": [
    "print \"old mean_f1_score:\", accuracy.mean_f1_score(svm_predictions, valid_labels)\n",
    "\n",
    "#overwrite the crappy svm predictions and use the bayes prediction\n",
    "svm_predictions[:,label_to_predict] = predictions\n",
    "#test new f1_score\n",
    "print \"new mean_f1_score:\", accuracy.mean_f1_score(svm_predictions, valid_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1., ...,  1.,  1.,  0.],\n",
       "       [ 0.,  1.,  1., ...,  1.,  1.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  1.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  1.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(data_dir + pickle_file, 'rb') as f:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
